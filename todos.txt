Environment properties:
* Either use configure_env() in vpg/utils.py or don't use it in all algos
* Create Agent variable for `is_discrete_env`. Add logic using this variable
* Rewards seem to be ep_rewards += reward for discrete and ep_rewards += reward[0] for continuous. Make sure to handle
each case
* In argparse, require env_name to be one of several choices.... parser.add_argument('env', choices=('CartPole-v0', ...)


Saving:
* Fix save_dir and load_dir to only work if we pass in the save_dir param
* Test saving and loading of models
* Fix saving of results.png. Why is it blank?
* Can we put this into a BaseAgent class instead?
* Make all methods save_models() instead of save_model()...It's okay if there's only 1 model to save, but
this will be more consistent
* Fix agents.save() for all VPG. We get an error!


Plotting:
* Fix plots! There are still several issues
* No consistency in plotting!
* Move logic to utils/utils.py where appropriate
* Update the main() method to call sample_trajectory() at a certain cadence for plotting


Other:
* Add critic, actor and total loss to plots
* Possibly add connection to tensorboard
* Right now I don't use max_step at all, even though it's in the argument parser

* change running_reward to be ep_rew if it's the first episode
* Move all non-configurable things to CONSTANTS

* Add self.total_steps as a variable for each Agent class
* Add self.cur_episode as a variable for each Agent class I currently do this for DDPG/TD3/SAC

* Add "required=True" to the parser, also, use "action=store_true" where relevant
* Think about whether it's cleaner to convert args to params using params = vars(args) after doing args.parser.parse_args()
* Clarify difference between episode and epochs
* Do we always need to do tf.convert_to_tensor()? Is it okay to keep things as numpy arrays instead? It would be much cleaner this way!
* For DDPG and possibly all Pendulum --> see why passing an action like [[0.44]] doesn't play well in env.step(action)...
* Lots of updates for Bandits!


Round 4:
* Dockerize


----------------------------------------------------------------------------------------------------------
--------------------------------------------------DONE----------------------------------------------------
----------------------------------------------------------------------------------------------------------

* Add run_agent() to Dyna-Q, Deep Dyna-Q
* Add, refactor, update run_agent() in all deep-RL agents
* Move methods to compute returns to own file
* Move obstacles_env.py into it's own folder
* Fix imports - use openrl/ as sources root
* Dimensions in replay buffer are hard-coded!! Change this, especially in DDPG
* Export some utils.py logic to another folder (e.g. ReplayBuffer, other functions)
* For Discrete(X) action spaces, action_dim = 1 always! Fix this in the fc_models and in the buffers, where I hard code
* For the fc_models, instead of taking "num_actions", take "action_dim"
* In the actor/critic functions, "num_inputs" is confusing, especially when the model inputs both
states and actions. Change "num_inputs" to "state_dims".
* either use tf.keras.backend.set_floatx('float32') on all scripts or remove from all
* Refactor models.py and utils.py so that they are at a higher directory so I don't have repeated code
* I think this is wrong on every single one ==>
-- if np.mean(latest_mean_rewards > best_mean_rewards):
-- It should just be => if latest_mean_rewards > best_mean_rewards .... We don't need the np.mean
* Create main() method
* Point everything to self.env instead of env
* I think sample_trajectory() has env.step() instead of self.env.step()!
* Remove "discrete" and "continuous" from the name of the critic methods. It's only the policy
that is discrete or continuous. The value function remains the same for both cases
* Add docstring to Replay Buffer
* Fix typehints for Replay Buffer
* The buffer should include "Done"
* Fix TD3 (and probably DDPG / SAC) targets. They should include "done" (i.e. 1-d) when determining
 whether or not to bootstrap!!

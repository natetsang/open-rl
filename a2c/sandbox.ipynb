{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "action_prob = tf.convert_to_tensor([[0.1, 0.9],\n",
    "                                    [0.2, 0.8],\n",
    "                                    [0.6, 0.4]])\n",
    "action = [1, 0, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "action_p = tf.convert_to_tensor([[0.4, 0.6]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.argmax(np.squeeze(action_p))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "action = [tf.argmax(ap) for ap in action_prob]\n",
    "action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.convert_to_tensor([tf.expand_dims(action_prob[i][a], 0) for i, a in enumerate(action)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[np.random.choice(2, p=np.squeeze(a_prob)) for a_prob in action_prob]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[prob[i][a]for i, a in enumerate(action)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.convert_to_tensor([tf.expand_dims(prob[0][a], 0) for a in action])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "action = [0, 1, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prob = tf.convert_to_tensor([[0.11, 0.12]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.argmax(prob[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "act = tf.reshape(action, (3,1))\n",
    "act"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.gather_nd(prob, act.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prob = tf.convert_to_tensor([[0.4, 0.6],\n",
    "                              [0.4, 0.6],\n",
    "                              [0.2, 0.8]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "action_prob = [tf.convert_to_tensor([[0.4],\n",
    "                                     [0.6],\n",
    "                                     [0.2]])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.convert_to_tensor([tf.expand_dims(prob[0][a], 0) for a in action])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.concat(rewards, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "next_values = tf.convert_to_tensor([[0.0547],  # env 1\n",
    "                                    [0.0462],  # env 2\n",
    "                                    [0.0507]]) # env 3\n",
    "\n",
    "rewards = [tf.convert_to_tensor([[-0.2831],   # env 1 step 1\n",
    "                                 [-0.1653],   # env 2 step 1\n",
    "                                 [-0.0018]]), # env 3 step 1\n",
    "           tf.convert_to_tensor([[-0.0015],   # env 1 step 2\n",
    "                                 [-0.0232],   # env 2 step 2\n",
    "                                 [-0.0027]])] # env 3 step 2\n",
    "\n",
    "masks = [tf.convert_to_tensor([[1.],\n",
    "                               [1.],\n",
    "                               [1.]]), \n",
    "         tf.convert_to_tensor([[1.],\n",
    "                               [1.],\n",
    "                               [1.]])]\n",
    "\n",
    "values = [tf.convert_to_tensor([[0.0546],\n",
    "                                [0.0456],\n",
    "                                [0.0512]]), \n",
    "          tf.convert_to_tensor([[0.0548],\n",
    "                                [0.0459],\n",
    "                                [0.0510]])]\n",
    "\n",
    "values_plus_next =  [tf.convert_to_tensor([[0.0546],\n",
    "                                           [0.0456],\n",
    "                                           [0.0512]]), \n",
    "                     tf.convert_to_tensor([[0.0548],\n",
    "                                           [0.0459],\n",
    "                                           [0.0510]]), \n",
    "                     tf.convert_to_tensor([[0.0547],\n",
    "                                           [0.0462],\n",
    "                                           [0.0507]])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "next_values = tf.convert_to_tensor([[0.0547]]) \n",
    "\n",
    "rewards = [tf.convert_to_tensor([[-0.2831]]), tf.convert_to_tensor([[-0.0015]])] # env 3 step 2\n",
    "\n",
    "masks = [tf.convert_to_tensor([[1.]]), tf.convert_to_tensor([[1.]])]\n",
    "\n",
    "values = [tf.convert_to_tensor([[0.0546]]), tf.convert_to_tensor([[0.0548]])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gae(next_value, rewards, masks, values, gamma=0.99, tau=0.95):\n",
    "#     print(\"NEXT_VALUE \", next_value)\n",
    "#     print(\"======\")\n",
    "#     print(\"REWARDS \", rewards)\n",
    "#     print(\"======\")\n",
    "#     print(\"MASKS \", masks)\n",
    "#     print(\"======\")\n",
    "#     print(\"VALUES \", values)\n",
    "    \n",
    "    values = values + [next_value]\n",
    "#     print(\"======\")\n",
    "#     print(\"VALUES + NEXT \", values)\n",
    "#     print(\"======\")\n",
    "#     print(\"======\")\n",
    "\n",
    "    gae = 0\n",
    "    returns = []  # list of advantages\n",
    "    for step in reversed(range(len(rewards))):\n",
    "        #delta is Bellman equation minus value of the state\n",
    "        # masks (is the episode done or not done)\n",
    "        # values[step + 1] the value that we're getting in the future\n",
    "        delta = rewards[step] + gamma * values[step + 1] * masks[step] - values[step]\n",
    "        #moving average of advantages discounted by gamma * tau\n",
    "        gae = delta + gamma * tau * masks[step] * gae\n",
    "        returns.insert(0, gae + values[step])\n",
    "        \n",
    "        print(\"rewards[step]: \", rewards[step])\n",
    "        print(\"values[step + 1]: \", values[step + 1])\n",
    "        print(\"masks[step]: \", masks[step])\n",
    "        print(\"values[step]: \", values[step])\n",
    "        print(\"delta: \", delta)\n",
    "        print(\"==============\")\n",
    "        print(\"==============\")\n",
    "\n",
    "    print(\"GAE\", returns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gae = 0\n",
    "delta = -0.0015 + 0.99 * 0.0547 * 1. - 0.0548\n",
    "gae = delta + 0.99 * 0.95 * 1.0 * 0\n",
    "gae + 0.0548"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_gae(next_values, rewards, masks, values, gamma=0.99, tau=0.95)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gae = 0\n",
    "delta = -0.0015 + 0.99 * 0.0547 * 1. - 0.0548\n",
    "gae = delta + 0.99 * 0.95 * 1.0 * 0\n",
    "gae + 0.0548"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_gae(next_values, rewards, masks, values, gamma=0.99, tau=0.95)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_gae(next_values, rewards, masks, values, gamma=0.99, tau=0.95)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.squeeze(tf.transpose(rewards))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "values = [tf.convert_to_tensor([[1], [2]])]\n",
    "values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dones = [tf.convert_to_tensor([[False], [False], [False]]),\n",
    "        tf.convert_to_tensor([[True], [True], [True]])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[[dones[s][w] for s in range(steps)] for w in range(workers)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state = [tf.convert_to_tensor([[1.1, 1.2, 1.3, 1.4], \n",
    "                                [2.1, 2.2, 2.3, 2.4]]),\n",
    "         tf.convert_to_tensor([[11.1, 11.2, 11.3, 11.4], \n",
    "                                [22.1, 22.2, 22.3, 22.4]]),\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "steps = 2\n",
    "workers = 2\n",
    "aa = [[state[s][w] for s in range(steps)] for w in range(workers)]\n",
    "aa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nate = [tf.convert_to_tensor([0.4, 0.6]), tf.convert_to_tensor([0.8, 0.2])]\n",
    "nate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.convert_to_tensor(nate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nate = tf.convert_to_tensor(aa)\n",
    "tf.squeeze(nate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reward = [tf.convert_to_tensor(1.0), tf.convert_to_tensor(1.0)]\n",
    "reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reward_history = tf.convert_to_tensor(reward)\n",
    "reward_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "done_history = tf.convert_to_tensor([False, False])\n",
    "done_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "critic_values = tf.convert_to_tensor([1.0, 2., 3.])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "1 - tf.cast(done_history, dtype=tf.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reward_history + (1 - tf.cast(done_history, dtype=tf.float32)) * 0.99 * critic_values[1:] - critic_values[:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "action_prob = tf.convert_to_tensor([[1.0, 0.0],\n",
    "                                   [0.0, 1.0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[np.random.choice(2, p=np.squeeze(a_prob)) for a_prob in action_prob]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probs = [\n",
    "    [tf.convert_to_tensor([0.4, 0.6]), tf.convert_to_tensor([0.8, 0.2])],\n",
    "    [tf.convert_to_tensor([0.5, 0.5]), tf.convert_to_tensor([0.0, 1.0])]\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[probs[i] * tf.math.log(tf.math.add(probs[i], 1e-8)) for i in range(1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "-0.17851482 +  -0.32188758"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[tf.reduce_sum(probs[i] * tf.math.log(tf.math.add(probs[i], 1e-8)), axis=1) for i in range(1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.math.log(probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.math.log(tf.convert_to_tensor([0.1, 0.9]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = [tf.convert_to_tensor([-0.69313216, -0.6931406]), \n",
    " tf.convert_to_tensor([-0.6931472 , -0.69189584]), \n",
    " tf.convert_to_tensor([-0.69314706, -0.6931128 ])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[100 * a[i] for i in range(3)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "a = [\n",
    "        [[tf.convert_to_tensor(0.4970687)], \n",
    "         [tf.convert_to_tensor(0.50135285)]], \n",
    "    \n",
    "         [[tf.convert_to_tensor(0.50135285)],\n",
    "         [tf.convert_to_tensor(0.50135285)]],\n",
    "         [[tf.convert_to_tensor(0.50135285)], \n",
    "         [tf.convert_to_tensor(0.50135285)]]\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ap_traj = [tf.convert_to_tensor(0.4), tf.convert_to_tensor(0.6)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "-tf.math.log(tf.convert_to_tensor(ap_traj)) * A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ap_traj = tf.convert_to_tensor([0.5, 0.6])\n",
    "A = tf.convert_to_tensor([1., 2.])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "-tf.math.log(tf.convert_to_tensor(ap_traj)) * A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.math.log(tf.reshape(a[0], [2])) * ADV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.math.log(tf.convert_to_tensor(a[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ADV = tf.convert_to_tensor([1.9053979 0.97962815])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bb = tf.convert_to_tensor(a[0])\n",
    "bb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reshape(bb, [bb.shape[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reshape(a[0], [a[0].shape[1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_advantages(next_value, reward_history, masks):\n",
    "    discounted_rewards = []\n",
    "    total_ret =  next_value * masks[-1]\n",
    "    for r in reward_history[::-1]:\n",
    "        total_ret = r + 0.99 * total_ret\n",
    "        discounted_rewards.append(total_ret)\n",
    "    discounted_rewards.reverse()\n",
    "    return discounted_rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[4.90099501, 3.9403989999999998, 2.9701, 1.99]"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calculate_advantages(1, [1,1,1,1], [1,1,1,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_advantages(next_value, reward_history, masks):\n",
    "\n",
    "    discounted_rewards = []\n",
    "    total_ret = next_value * masks[-1]\n",
    "    for r in reward_history[::-1]:\n",
    "        total_ret = r + 0.99 * total_ret\n",
    "        discounted_rewards.insert(0, total_ret)\n",
    "#     discounted_rewards.reverse()\n",
    "    \n",
    "    return discounted_rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[4.90099501, 3.9403989999999998, 2.9701, 1.99]"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calculate_advantages(1, [1,1,1,1], [1,1,1,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_advantages(next_value, rewards, masks):\n",
    "    rewards = rewards + [next_value]\n",
    "    gae = 0\n",
    "    returns = []\n",
    "    for step in reversed(range(len(rewards) -1)):\n",
    "        delta = rewards[step] + 0.99 * rewards[step + 1] * masks[step] \n",
    "        gae = delta + 0.99 * masks[step] * gae\n",
    "        returns.insert(0, gae)\n",
    "    return returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[7.84139401, 5.910499, 3.9600999999999997, 1.99]"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calculate_advantages(1, [1,1,1,1], [1,1,1,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_advantages(next_value, rewards, masks):\n",
    "    rewards = rewards + [next_value]\n",
    "    returns = []\n",
    "    rsum = 0\n",
    "    for step in reversed(range(len(rewards) - 1)):\n",
    "#         print(f\"rewards[step +1] + 0.99 * rsum * masks[step]\")\n",
    "#         print(f\"{rewards[step +1]} + {0.99} * {rsum} * {masks[step]}\")\n",
    "        rsum = rewards[step +1] + 0.99 * rsum * masks[step]\n",
    "#         print(\"RSUM: \", rsum)\n",
    "        returns.insert(0, rsum)\n",
    "#         print(\"Returns: \", returns)\n",
    "    return returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3.9403989999999998, 2.9701, 1.99, 1.0]"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calculate_advantages(1, [1,1,1,1], [1,1,1,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "5 + 0.99 * 0 * 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

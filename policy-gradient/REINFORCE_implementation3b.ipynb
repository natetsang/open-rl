{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# REINFORCE\n",
    "Training takes quite a while for this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import time\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate environment\n",
    "env = gym.make(\"CartPole-v1\")\n",
    "\n",
    "observation_dim = env.observation_space.shape[0]\n",
    "action_dim = env.action_space.n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_policy_network(input_size, output_size):\n",
    "    inputs = tf.keras.layers.Input(input_size, name='input')\n",
    "    dense1 = tf.keras.layers.Dense(32, activation='relu', name='hidden')(inputs)\n",
    "    outputs = tf.keras.layers.Dense(output_size, activation='softmax', name='prob_outputs')(dense1)\n",
    "    model = tf.keras.Model(inputs=inputs, outputs=outputs)\n",
    "    \n",
    "    optimizer=tf.keras.optimizers.Adam(lr=1e-3)\n",
    "    return model, optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discount_rewards(rewards, discount_rate=0.99, normalize_rewards=False):\n",
    "    rewards_to_go = list()\n",
    "    cumulative_rewards = 0.0\n",
    "\n",
    "    for r in rewards[::-1]:\n",
    "        cumulative_rewards = r + discount_rate * cumulative_rewards\n",
    "        rewards_to_go.append(cumulative_rewards)\n",
    "    rewards_to_go.reverse()\n",
    "\n",
    "    if normalize_rewards:\n",
    "        mean = np.mean(rewards_to_go)\n",
    "        std = np.std(rewards_to_go)\n",
    "        rewards_to_go = (rewards_to_go - mean) / std\n",
    "    return np.array(rewards_to_go)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_action(model, state):\n",
    "    state = state.reshape((1, -1))\n",
    "    action_prob = model.predict(state, batch_size=1).flatten()\n",
    "    selected_action = np.random.choice(action_dim, 1, p=action_prob)[0]\n",
    "    return selected_action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_trajectories(env, model):\n",
    "    batch_rewards = []\n",
    "    batch_obs = []\n",
    "    batch_actions = []\n",
    "    obs = env.reset()\n",
    "    \n",
    "    done = False\n",
    "    while not done:\n",
    "        action= get_action(model, obs)\n",
    "        batch_obs.append(obs)\n",
    "        obs, reward, done, _ = env.step(action)\n",
    "        \n",
    "        batch_actions.append(action)\n",
    "        batch_rewards.append(reward)\n",
    "    \n",
    "    # postprocess\n",
    "    batch_obs = np.vstack(batch_obs)\n",
    "    batch_actions = np.array(batch_actions).reshape((len(batch_actions), 1))  \n",
    "    batch_rewards = np.array(batch_rewards)\n",
    "    return batch_obs, batch_actions, batch_rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(env, model):\n",
    "    states, actions, rewards = generate_trajectories(env, policy_model)\n",
    "    discounted_rewards = discount_rewards(rewards)\n",
    "    discounted_rewards = tf.convert_to_tensor(tf.cast(discounted_rewards, dtype=tf.float32))\n",
    "    \n",
    "    # Forward pass\n",
    "    with tf.GradientTape() as tape:\n",
    "        action_probs = model(states)\n",
    "        action_probs = tf.gather_nd(action_probs, actions, batch_dims=1)\n",
    "        action_probs = tf.clip_by_value(action_probs, 1e-8, 1-1e-8)\n",
    "        \n",
    "        loss = - tf.reduce_sum(tf.multiply(tf.math.log(action_probs), discounted_rewards))\n",
    "    \n",
    "    # Backwards pass\n",
    "    gradients = tape.gradient(loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "    \n",
    "    episode_reward = np.sum(rewards)\n",
    "    return episode_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "NUM_EPISODES = 650\n",
    "\n",
    "policy_model, optimizer = build_policy_network(observation_dim, action_dim)\n",
    "\n",
    "episode_rewards = list()\n",
    "\n",
    "start = time.time()\n",
    "for episode in range(NUM_EPISODES):\n",
    "    ep_rewards = train(env, policy_model)\n",
    "    episode_rewards.append(ep_rewards)\n",
    "    if episode <= 50:\n",
    "        print(f'episode {episode}/{NUM_EPISODES} - reward: {ep_rewards}')\n",
    "    if episode > 50:\n",
    "        print(f'episode {episode}/{NUM_EPISODES} - reward: {ep_rewards} - last 50 avg: {np.mean(episode_rewards[-50:])}')\n",
    "end = time.time()\n",
    "print(f'Training time: {int(round(end - start))} seconds.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = [i for i in range(605)]\n",
    "plt.plot(x, episode_rewards)\n",
    "smoothed_fn = np.poly1d(np.polyfit(x, episode_rewards, 3))\n",
    "plt.plot(x, smoothed_fn(x), linestyle = '-')\n",
    "    \n",
    "plt.xlabel(\"episode number\")\n",
    "plt.ylabel(\"reward\")\n",
    "plt.title(\"Simple PG Training\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
